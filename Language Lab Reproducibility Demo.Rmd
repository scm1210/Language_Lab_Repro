---
title: "Language Lab Reproducibility Demo"
author: "Steven Mesquiti"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    highlight: tango
    theme: united
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---
```{r setup, include=FALSE, message=FALSE}
# set chunk options for the document
# include=FALSE means that this chunk will not show up in the report

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = F #let's us save old models. only wanna do this when models are finalized
                      , dpi = 150, fig.path = "CEO_figs/") 
# echo = TRUE means that the source code will be displayed
# message = FALSE suppresses messages
# warning = FALSE suppresses warnings
# cache = FALSE recompiles from scratch each time you knit 
# dpi = 150 sets the figure resolution
# fig.path specifies a directory where figures will be output

options(scipen = 999) #turn off scientific notation
options(repos = c(CRAN = "https://cran.rstudio.com/"))
set.seed(65) #set seed for random number generation
```

This tutorial uses data and reproduces a subset of analyses reported in the following manuscript:

[Mesquiti & Seraj. (Preprint) The Psychological Impacts of the COVID-19 Pandemic on Corporate Leadership](https://psyarxiv.com/kvar9/)


The COVID-19 pandemic sent shockwaves across the fabric of our society. Examining the impact of the pandemic on business leadership is particularly important to understanding how this event affected their decision-making. The present study documents the psychological effects of the COVID-19 pandemic on chief executive officers (CEOs). This was accomplished by analyzing CEOs’ language from quarterly earnings calls (N = 19,536) for a year before and after lockdown. CEOs had large shifts in language in the months immediately following the start of the pandemic lockdowns. Analytic thinking plummeted after the world went into lockdown, with CEOs’ language becoming less technical and more personal and intuitive. In parallel, CEOs’ language showed signs of increased cognitive load, as they were processing the effect of the pandemic on their business practices. Business leaders’ use of collective-focused language (we-usage) dropped substantially after the pandemic began, perhaps suggesting CEOs felt disconnected from their companies. Self-focused (I-usage) language increased, showing the increased preoccupation of business leaders. The size of the observed shifts in language during the pandemic also dwarfed responses to other events that occurred dating back to 2010, with the effect lasting around seven months.


# Prep data {.tabset}

## Load necessary packages and set Working Directory
```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,zoo,lubridate,plotrix,ggpubr, caret, broom, kableExtra, reactable, effsize, install = T)
setwd("~/Desktop/Language_Lab_Repro")
```


## Define aesthetics
```{r}
palette_map = c("#3B9AB2", "#EBCC2A", "#F21A00")
palette_condition = c("#ee9b00", "#bb3e03", "#005f73")

plot_aes = theme_classic() +
  theme(text = element_text(size = 16, family = "Futura Medium")) + 
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  theme(plot.title.position = 'plot', 
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)) + 
  theme(axis.text=element_text(size=16),
        axis.title=element_text(size=20,face="bold"))+
  theme(plot.title.position = 'plot', 
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20)) +
  theme(axis.text=element_text(size = 14),
        axis.title=element_text(size = 20,face="bold"))
```

## Write our Table Funcions
```{r}
baseline_ttest <- function(ttest_list) {
  # Extract relevant information from each test and store in a data frame
  ttest_df <- data.frame(
    Group1 = seq(0,0,1),
    Group2 = seq(1,24,1),
    t = sapply(ttest_list, function(x) x$statistic),
    df = sapply(ttest_list, function(x) x$parameter),
    p_value = sapply(ttest_list, function(x) x$p.value)
  )
  
  # Format p-values as scientific notation
  ttest_df$p_value <- format(ttest_df$p_value, scientific = T)
  
  # Rename columns
  colnames(ttest_df) <- c("t", "t + 1 ", "t-value", "Degrees of Freedom", "p-value")
  
  # Create table using kableExtra
  kable(ttest_df, caption = "Summary of Welch's t-Tests", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}

post_pandemic_summary <- function(ttest_list) {
  # Extract relevant information from each test and store in a data frame
  ttest_df <- data.frame(
    Group1 = seq(12,23,1),
    Group2 = seq(13,24,1),
    t = sapply(ttest_list, function(x) x$statistic),
    df = sapply(ttest_list, function(x) x$parameter),
    p_value = sapply(ttest_list, function(x) x$p.value)
  )
  
  # Format p-values as scientific notation
  ttest_df$p_value <- format(ttest_df$p_value, scientific = T)
  
  # Rename columns
  colnames(ttest_df) <- c("t", "t + 1 ", "t-value", "Degrees of Freedom", "p-value")
  
  # Create table using kableExtra
  kable(ttest_df, caption = "Summary of Welch's t-Tests", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}



baseline_cohen_d <- function(cohen_d_list) {
  # Extract relevant information from each test and store in a data frame
  cohen_d_df <- data.frame(
    Group1 = seq(0,0,1),
    Group2 = seq(1,24,1),
    Cohen_d = sapply(cohen_d_list, function(x) x$estimate)
  )
  
  # Rename columns
  colnames(cohen_d_df) <- c("t", "t + 1", "Cohen's d")
  
  # Create table using kableExtra
  kable(cohen_d_df, caption = "Summary of Cohen's D", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}

post_cohen_d <- function(cohen_d_list) {
  # Extract relevant information from each test and store in a data frame
  cohen_d_df <- data.frame(
    Group1 = seq(12,23,1),
    Group2 = seq(13,24,1),
    Cohen_d = sapply(cohen_d_list, function(x) x$estimate)
  )
  
  # Rename columns
  colnames(cohen_d_df) <- c("t", "t+1", "Cohen's d")
  
  # Create table using kableExtra
  kable(cohen_d_df, caption = "Summary of Cohen's D", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}

baseline_mean_diff <- function(mean_diff_list) {
  # Extract relevant information from each mean difference calculation and store in a data frame
  mean_diff_df <- data.frame(
    Group1 = seq(0,0,1),
    Group2 = seq(1,24,1),
    mean_diff = mean_diff_list
  )
  
  # Rename columns
  colnames(mean_diff_df) <- c("t", "t+1", "Mean Difference")
  
  # Create table using kableExtra
  kable(mean_diff_df, caption = "Summary of Mean Differences", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}


post_mean_diff <- function(mean_diff_list) {
  # Extract relevant information from each mean difference calculation and store in a data frame
  mean_diff_df <- data.frame(
    Group1 = seq(12,23,1),
    Group2 = seq(13,24,1),
    mean_diff = mean_diff_list
  )
  
  # Rename columns
  colnames(mean_diff_df) <- c("t", "t+1", "Mean Difference")
  
  # Create table using kableExtra
  kable(mean_diff_df, caption = "Summary of Mean Differences", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}

```

## Load in the Data 
```{r}
data  <-  read_csv("Big_CEO.csv") #read in the data

data <- data["2019-03-01"<= data$Date & data$Date <= "2021-04-01",] #subsetting covid dates 

data <- data %>% filter(WC<=5400) %>% #filter out based on our exclusion criteria
  filter(WC>=25)

data$month_year <- format(as.Date(data$Date), "%Y-%m") #reformat 

data_tidy <- data %>% dplyr::select(Date, Speaker, Analytic, cogproc,allnone,we,i,emo_anx) %>%
  mutate(Date = lubridate::ymd(Date),
         time_month = as.numeric(Date - ymd("2019-03-01")) / 30, #centering at start of march
         time_month_quad = time_month * time_month) #making our quadratic term

data_tidy$Date_off <- floor(data_tidy$time_month) #rounding off dates to whole months using ceiling function (0 = 2019-03, 24 = 2021-04)
data_tidy$Date_covid <- as.factor(data_tidy$Date_off) #factorize

```

## Create Tidy Data for Graphs

```{r,}
df <- read_csv("Big_CEO.csv")#put code here to read in Big CEO data
df <- df %>% filter(WC<=5400)   %>% 
  filter(WC>=25)

df$month_year <- format(as.Date(df$Date), "%Y-%m") ###extracting month and year to build fiscal quarter graphs, need a new variable bc if not it'll give us issues

df2 <- df %>%#converting our dates to quarterly dates 
  group_by(month_year) %>% ###grouping by the Top100 tag and date 
  summarise_at(vars("Date","WC","Analytic","cogproc",'we','i'),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest

df2 <- df2["2019-01"<= df2$month_year & df2$month_year <= "2021-03",] #covid dates 
```

# Write our Stats Functions {.tabset}

We were interested in how language changed relative to baseline one year pre-pandemic, as well as how language changed after the Pandemic. 

As a result we ran two separate set of analyses comparing t(time zero) to t[i] and t(12 months after our centered data point) to t + 1. The groups you see will be centered on 03/2019. That is, 12 = 03/2020, 13 = 04/2020, etc. etc.

## Analytic Thinking 
```{r}
analytic_my.t = function(fac1, fac2){
  t.test(data_tidy$Analytic[data_tidy$Date_covid==fac1], 
         data_tidy$Analytic[data_tidy$Date_covid==fac2])
} #writing our t-test function to compare t to t[i] 

analytic_my.d = function(fac1, fac2){
  cohen.d(data_tidy$Analytic[data_tidy$Date_covid==fac1], 
          data_tidy$Analytic[data_tidy$Date_covid==fac2])
} #function for cohen's d

analytic_mean <-  function(fac1, fac2){
  mean(data_tidy$Analytic[data_tidy$Date_covid==fac1])- 
    mean(data_tidy$Analytic[data_tidy$Date_covid==fac2])
} #function to do mean differences

```


## Cognitive Processing 
```{r}
cogproc_my.t = function(fac1, fac2){
  t.test(data_tidy$cogproc[data_tidy$Date_covid==fac1], 
         data_tidy$cogproc[data_tidy$Date_covid==fac2])
} #writing our t-test function to compare t to t[i] 


cogproc_my.d = function(fac1, fac2){
  cohen.d(data_tidy$cogproc[data_tidy$Date_covid==fac1], 
          data_tidy$cogproc[data_tidy$Date_covid==fac2])
} #function for cohen's d

cogproc_mean <-  function(fac1, fac2){
  mean(data_tidy$cogproc[data_tidy$Date_covid==fac1])- 
    mean(data_tidy$cogproc[data_tidy$Date_covid==fac2])
} #function to do mean differences
```

## I-words
```{r}
i_my.t = function(fac1, fac2){
  t.test(data_tidy$i[data_tidy$Date_covid==fac1], 
         data_tidy$i[data_tidy$Date_covid==fac2])
} #writing our t-test function to compare t to t + 1 

i_my.d = function(fac1, fac2){
  cohen.d(data_tidy$i[data_tidy$Date_covid==fac1], 
          data_tidy$i[data_tidy$Date_covid==fac2])
} #function for cohen's d


i_mean <-  function(fac1, fac2){
  mean(data_tidy$i[data_tidy$Date_covid==fac1])- 
    mean(data_tidy$i[data_tidy$Date_covid==fac2])
} #function to do mean differences

```


## We-words
```{r}
we_my.t = function(fac1, fac2){
  t.test(data_tidy$we[data_tidy$Date_covid==fac1], 
         data_tidy$we[data_tidy$Date_covid==fac2])
} 

we_my.d = function(fac1, fac2){
  cohen.d(data_tidy$we[data_tidy$Date_covid==fac1], 
          data_tidy$we[data_tidy$Date_covid==fac2])
} #function for cohen's d

we_mean <-  function(fac1, fac2){
  mean(data_tidy$we[data_tidy$Date_covid==fac1])- 
    mean(data_tidy$we[data_tidy$Date_covid==fac2])
} #function to do mean differences
```


## Tidy data
Data transformations

* None

Exclusions

* Excluded texts that were shorter than ** 25 words ** and greater than ** 5,400 words **!

# Summary of the Data {.tabset}

## Range of Dates

```{r}
range(data$Date)
```


## Number of Speakers

```{r}
speakers <- data %>%
  select(Speaker) %>%
  unique() %>%
  dplyr::summarize(n = n()) %>%
  reactable::reactable(striped = TRUE)
speakers
```

## Number of Transcripts

```{r}
transcripts <- data %>%
  select(1) %>%
  dplyr::summarize(n = n()) %>%
  reactable::reactable(striped = TRUE)
transcripts
```

## Mean Word Count 

```{r}
word_count <- data %>%
  select(WC) %>%
  dplyr::summarize(mean = mean(WC)) %>%
  reactable::reactable(striped = TRUE)
word_count
```


# How did language change after the Pandemic? 

## Analytic Thinking {.tabset}

### T-test 

```{r}
analytic_ttest<- mapply(analytic_my.t,seq(12,23,1), seq(13,24,1),SIMPLIFY=F) #compare t (first parantheses) to t[i] (second parentheses)increasing by 1
post_pandemic_summary(analytic_ttest)
```


### Cohen's D

```{r}
analytic_d <- mapply(analytic_my.d,seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) 
post_cohen_d(analytic_d)
```

### Mean Differences

```{r}
analytic_meandiff <- mapply(analytic_mean, seq(12,23,1), seq(13,24,1)) #across all of the months comparing to time zero
post_mean_diff(analytic_meandiff)
```

## Cogproc {.tabset}

### T-test

```{r}
cogproc_ttest <-mapply(cogproc_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1
post_pandemic_summary(cogproc_ttest)
```

### Cohen's D

```{r}
cogproc_d <-mapply(cogproc_my.d, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)
post_cohen_d(cogproc_d)
```

### Mean Differences

```{r}
cogproc_meandiff <- mapply(cogproc_mean, seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2019]across all of the months
post_mean_diff(cogproc_meandiff)
```

## I-words {.tabset}

### T-test

```{r}
i_ttest <- mapply(i_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first paratheses) to t[i] (second parentheses) increasing by 1
post_pandemic_summary(i_ttest)
```

### Cohen's D

```{r}
i_d <- mapply(i_my.d,seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)
post_cohen_d(i_d)
```

### Mean Differences

```{r}
i_meandiff <- mapply(i_mean,seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2020]across all of the months
post_mean_diff(i_meandiff)
```

## We-words {.tabset}

### T-test 

```{r}
we_ttest <- mapply(we_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1
post_pandemic_summary(we_ttest)
```

### Cohen's D

```{r}
we_d <- mapply(we_my.d, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)
post_cohen_d(we_d)
```


### Mean Differences
```{r}
we_meandiff <- mapply(we_mean, seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2020]across all of the months
post_mean_diff(we_meandiff)
```

# How did language change relative to baseline (one year before the pandemic; 03/2019)?

## Analytic Thining {.tabset}

### T-test 

```{r}
analytic_ttest_baseline <-mapply(analytic_my.t,0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parantheses) to t[i] (second parentheses)increasing by 1
baseline_ttest(analytic_ttest_baseline)
```


### Cohen's D

```{r}
analytic_D_baseline <- mapply(analytic_my.d,0, seq(1,24,1),SIMPLIFY=FALSE) 
baseline_cohen_d(analytic_D_baseline)
```

### Mean Differences

```{r}
analytic_mean_baseline <- mapply(analytic_mean, 0, seq(1,24,1)) #across all of the months comparing to time zero
baseline_mean_diff(analytic_mean_baseline)
```

## Cogproc {.tabset}

### T-test

```{r}
cogproc_ttest_baseline <- mapply(cogproc_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1
baseline_ttest(cogproc_ttest_baseline)
```

### Cohen's D

```{r}
cogproc_D_baseline <- mapply(cogproc_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)
baseline_cohen_d(cogproc_D_baseline)
```

### Mean Differences

```{r}
cogproc_mean_baseline <- mapply(cogproc_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months
baseline_mean_diff(cogproc_meandiff)
```

## I-words {.tabset}

### T-test

```{r}
i_ttest_baseline <- mapply(i_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first paratheseses) to t[i] (second parentheses) increasing by 1
baseline_ttest(i_ttest_baseline)
```

### Cohen's D

```{r}
i_D_baseline <- mapply(i_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)
baseline_cohen_d(i_D_baseline)
```

### Mean Differences

```{r}
i_mean_baseline <- mapply(i_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months
baseline_mean_diff(i_mean_baseline)
```

## We-words {.tabset}

### T-test 

```{r}
we_ttest_baseline <- mapply(we_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1
baseline_ttest(we_ttest_baseline)
```

### Cohen's D

```{r}
we_D_baseline <- mapply(we_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)
baseline_cohen_d(we_D_baseline)
```


### Mean Differences
```{r}
we_mean_baseline <- mapply(we_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months
baseline_mean_diff(we_mean_baseline)
```

# Build our Graphs {.tabset}

## Analytic Thinking 

```{r fig.height=6, fig.width=6}
Analytic <- ggplot(data=df2, aes(x=Date_mean, y=Analytic_mean, group=1)) +
  geom_line(colour = "dodgerblue3") +
  scale_x_date(date_breaks = "3 month", date_labels = "%Y-%m") +
  geom_ribbon(aes(ymin=Analytic_mean-Analytic_std.error, ymax=Analytic_mean+Analytic_std.error), alpha=0.2) +
  ggtitle("Analytic Thinking") +
  labs(x = "Month", y = 'Standardized score') +
  plot_aes + #here's our plot aes object
  geom_vline(xintercept = as.numeric(as.Date("2020-03-01")), linetype = 1) +
  geom_rect(data = df2, #summer surge
            aes(xmin = as.Date("2020-06-15", "%Y-%m-%d"), 
                xmax = as.Date("2020-07-20",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009) +
  geom_rect(data = df2, #winter surge
            aes(xmin = as.Date("2020-11-15", "%Y-%m-%d"), 
                xmax = as.Date("2021-01-01",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009)
Analytic <- Analytic + annotate(geom="text",x=as.Date("2020-07-01"),
                                y=43,label="Summer 2020 surge", size = 3) + 
  annotate(geom="text",x=as.Date("2020-12-03"),
           y=43,label="Winter 2020 surge", size = 3)
Analytic
```

## Cogproc
```{r fig.height=6, fig.width=6}
Cogproc <- ggplot(data=df2, aes(x=Date_mean, y=cogproc_mean, group=1)) +
  geom_line(colour = "dodgerblue3") +
  scale_x_date(date_breaks = "3 month", date_labels = "%Y-%m") +
  geom_ribbon(aes(ymin=cogproc_mean-cogproc_std.error, ymax=cogproc_mean+cogproc_std.error), alpha=0.2) +
  ggtitle("Cognitive Processing") +
  labs(x = "Month", y = '% Total Words') +
  plot_aes + #here's our plot aes object
  geom_vline(xintercept = as.numeric(as.Date("2020-03-01")), linetype = 1) +
  geom_rect(data = df2, #summer surge
            aes(xmin = as.Date("2020-06-15", "%Y-%m-%d"), 
                xmax = as.Date("2020-07-20",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009) +
  geom_rect(data = df2, #winter surge
            aes(xmin = as.Date("2020-11-15", "%Y-%m-%d"), 
                xmax = as.Date("2021-01-01",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009)
Cogproc <- Cogproc + annotate(geom="text",x=as.Date("2020-07-01"),
                                y=12.5,label="Summer 2020 surge", size = 3) + 
  annotate(geom="text",x=as.Date("2020-12-03"),
           y=12.5,label="Winter 2020 surge", size = 3)
Cogproc
```

## I-words
```{r fig.height=6, fig.width=6}
i <- ggplot(data=df2, aes(x=Date_mean, y=i_mean, group=1)) +
  geom_line(colour = "dodgerblue3") +
  scale_x_date(date_breaks = "3 month", date_labels = "%Y-%m") +
  geom_ribbon(aes(ymin=i_mean-i_std.error, ymax=i_mean+i_std.error), alpha=0.2) +
  ggtitle("I-usage") +
  labs(x = "Month", y = '% Total Words') +
  plot_aes + #here's our plot aes object
  geom_vline(xintercept = as.numeric(as.Date("2020-03-01")), linetype = 1) +
  geom_rect(data = df2, #summer surge
            aes(xmin = as.Date("2020-06-15", "%Y-%m-%d"), 
                xmax = as.Date("2020-07-20",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009) +
  geom_rect(data = df2, #winter surge
            aes(xmin = as.Date("2020-11-15", "%Y-%m-%d"), 
                xmax = as.Date("2021-01-01",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009)
i <- i + annotate(geom="text",x=as.Date("2020-07-01"),
                                y=1.95,label="Summer 2020 surge", size = 3) + 
  annotate(geom="text",x=as.Date("2020-12-03"),
           y=1.95,label="Winter 2020 surge", size = 3)
i
```

## We-words
```{r fig.height=6, fig.width=6}
we <- ggplot(data=df2, aes(x=Date_mean, y=we_mean, group=1)) +
  geom_line(colour = "dodgerblue3") +
  scale_x_date(date_breaks = "3 month", date_labels = "%Y-%m") +
  geom_ribbon(aes(ymin=we_mean-we_std.error, ymax=we_mean+we_std.error), alpha=0.2) +
  ggtitle("We-usage") +
  labs(x = "Month", y = '% Total Words') +
  plot_aes + #here's our plot aes object
  geom_vline(xintercept = as.numeric(as.Date("2020-03-01")), linetype = 1) +
  geom_rect(data = df2, #summer surge
            aes(xmin = as.Date("2020-06-15", "%Y-%m-%d"), 
                xmax = as.Date("2020-07-20",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009) +
  geom_rect(data = df2, #winter surge
            aes(xmin = as.Date("2020-11-15", "%Y-%m-%d"), 
                xmax = as.Date("2021-01-01",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009)
we <- we + annotate(geom="text",x=as.Date("2020-07-01"),
                                y=6.5,label="Summer 2020 surge", size = 3) + 
  annotate(geom="text",x=as.Date("2020-12-03"),
           y=6.5,label="Winter 2020 surge", size = 3)
we
```

## Tie them all together
```{r,fig.height=14, fig.width=14}
graphs <- ggpubr::ggarrange(Analytic,Cogproc,i,we,ncol=2, nrow=2, common.legend = TRUE, legend = "bottom")
annotate_figure(graphs,
                top = text_grob("CEOs' Language Change",  color = "black", face = "bold", size = 20),
                bottom = text_grob("Note. Vertical Line Represents the onset of the pandemic. \n\ Horizontal shading represents Standard Error. Vertical bars represent virus surges."
                                   , color = "Black",
                                   hjust = 1.1, x = 1, face = "italic", size = 16))
```

# Package Citations 
```{r}
report::cite_packages()
```

All credit goes to the great Dani Cosme for teaching me how to make these! You can find her [github](https://github.com/dcosme) here!